# Research-Practicum-I

Title of the project
Unmasking Falsehood with NLP and Deep Learning in New Classification Using Popular Social Media Network.

The threat Fake News poses on the integrity of information dissemination is on the rise. There is the need to unmask some of this falsehood before it causes more harm to society in general.

Some data science task to be completed in the project includes
Text classification such as multi-class classification.
Sentiment Analysis.
Topic modelingAnomaly detection
Fake new detection
World cloud 

Data would be gotten from some popular social media websites such as facebook, reddit etc and also from some articles.

The data will be 
preprocessed 
Feature extraction
Training of model
Validation of the model
Hyperparameter tuning

Some difficulties I am anticipating are in the area of data collection example includes collection tweets which coils take some time. This could be overcomed using other platforms that are easily accessible.

I plan on completing the project in 6 weeks, this will be in order
Data collection
preprocessed 
Feature extraction
Training of model
Validation of the model
Hyperparameter tuning


A preview of the dataset is shown as follows
![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/e192e33f-f25d-4c4d-bd09-6d8b4541eef1)

THE CLEANING AND PREPROCESSING PROCESS
The dataset has some unwanted feature that were taken out for the smooth running of the model training. This include the id and the title of the news that was release.

![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/6026da58-665c-455e-84d5-0a8ef3ffd63b)
The label feature was reassigned numbers for easy training.

EXPLORATORY ANALYSIS 
![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/74a37989-a08a-4eed-81c5-d530be483bd4)
The description of the dataset could be seen. We look at the length of the title of the news.

![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/1ad09e84-90f4-47cd-b129-1f5375a91937)
This represent the length of the text that will be analyzed. More than 4000 characters sample represent the total sample analyzed.

![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/39b4eaac-8c1a-4332-95d4-e6505cca0dbe)
The dataset was reshuffled before analysis to prevent overfitting.

WORDCLOUD BEFORE STREMMING PROCESS
![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/5fd9b0f6-f8db-42d4-aea8-8a5a16c2a32b)

WORDCLOUD AFTER STREMMING PROCESS
![image](https://github.com/adeyeyealex/Research-Practicum-I/assets/77544400/67e5c3ac-eb12-4e91-a422-f45954fd3077)


